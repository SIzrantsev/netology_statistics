{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture_5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4hzAP6kG0qCb",
        "J0t8bCoH8rIS"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yustinaivanova/netology_statistics/blob/master/lecture_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB1egUSX0gH-",
        "colab_type": "text"
      },
      "source": [
        "# Кейс-стади -2. Определение СПАМа в тексте"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01apPJT-zhCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import t\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "from scipy.stats import pearsonr\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvItYzkj6nUK",
        "colab_type": "text"
      },
      "source": [
        "Датасет для определения СПАМа в тексте."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df9gsOk36YpQ",
        "colab_type": "code",
        "outputId": "f317cf4e-4660-4576-a5a0-283345b0913f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "url='http://yustiks.ru/dataset/SPAM_text.csv'\n",
        "s=requests.get(url).content\n",
        "data=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Category                                            Message\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiBSfYAu7E3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[\"Category\"] = [1 if each == \"spam\" else 0 for each in data[\"Category\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxJF-FT57GgK",
        "colab_type": "code",
        "outputId": "ead10828-30fa-4951-f86c-567a280ecb7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Category                                            Message\n",
              "0         0  Go until jurong point, crazy.. Available only ...\n",
              "1         0                      Ok lar... Joking wif u oni...\n",
              "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3         0  U dun say so early hor... U c already then say...\n",
              "4         0  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9Vu6XzsBfWw",
        "colab_type": "text"
      },
      "source": [
        "Как на основе текста предсказать, что он является СПАМом? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwcux5EgBqJN",
        "colab_type": "text"
      },
      "source": [
        "# Словарь BAG-of-words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAuXKIzSB-82",
        "colab_type": "text"
      },
      "source": [
        "Создаем слова. На основе слов пишем для каждого текста словарь, где каждое слово - это ключ, а значение ключа - это сколько раз встречается данное слова в данном тексте."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyjsN0ZqPPgn",
        "colab_type": "text"
      },
      "source": [
        "Как пример: рассмотрим 1 строку из датасета.\n",
        "\n",
        "\n",
        "\n",
        "*   Удалим все символы, не являющимися латинскими буквами\n",
        "*   Заглавные буквы меняем на строчные\n",
        "*   Разделим текст на слова\n",
        "*   В каждом слове выделяем корень слова\n",
        "*   Создаем список всех слов\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq4UlSZkNH-E",
        "colab_type": "code",
        "outputId": "51d6c960-89a8-47fd-c48c-3f4e4f6f1e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import re\n",
        "nlp_data = str(data.loc[2, 'Message'])\n",
        "print(nlp_data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E_4QMsONv3v",
        "colab_type": "text"
      },
      "source": [
        "Удаление всех не латинских букв:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRM_Ykz3NJvK",
        "colab_type": "code",
        "outputId": "2ac07c01-7373-415d-e081-bdbddeab0db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "nlp_data = re.sub(\"[^a-zA-Z]\",\" \",nlp_data)\n",
        "print(nlp_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Free entry in   a wkly comp to win FA Cup final tkts   st May       Text FA to       to receive entry question std txt rate T C s apply            over   s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_TIDOgsN-Y_",
        "colab_type": "text"
      },
      "source": [
        "Во всех словах заглавные буквы меняем на строчные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzy8kpblN5aE",
        "colab_type": "code",
        "outputId": "414d8639-eb93-4951-d90e-ff176577e27d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "nlp_data = nlp_data.lower()\n",
        "print(nlp_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "free entry in   a wkly comp to win fa cup final tkts   st may       text fa to       to receive entry question std txt rate t c s apply            over   s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ibxAPXnOgvj",
        "colab_type": "text"
      },
      "source": [
        "Переводим текст в отдельные слова"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOhX6nvMOPJD",
        "colab_type": "code",
        "outputId": "63faff23-3a37-495d-d4ba-7bd2ade34cbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import nltk as nlp\n",
        "nlp.download('punkt')\n",
        "nlp_data = nlp.word_tokenize(nlp_data)\n",
        "print(nlp_data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['free', 'entry', 'in', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'to', 'to', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 't', 'c', 's', 'apply', 'over', 's']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dpudpmrO_Tp",
        "colab_type": "text"
      },
      "source": [
        "Ищем корень каждого слова"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbPUYfsTOo9o",
        "colab_type": "code",
        "outputId": "8af4f62e-66e9-41bb-99f5-e4933eb647a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "nlp.download('wordnet')\n",
        "lemma = nlp.WordNetLemmatizer()\n",
        "nlp_data = [lemma.lemmatize(word) for word in nlp_data]\n",
        "print(nlp_data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "['free', 'entry', 'in', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'to', 'to', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 't', 'c', 's', 'apply', 'over', 's']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40keBPrTCLqB",
        "colab_type": "text"
      },
      "source": [
        "Добавляем все найденные слова в список"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O1MxC11PJ70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_data = \" \".join(nlp_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWpIjfZiQk8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "description_list = []\n",
        "for description in data[\"Message\"]:\n",
        "    description = re.sub(\"[^a-zA-Z]\",\" \",description)\n",
        "    description = description.lower()\n",
        "    description = nlp.word_tokenize(description)\n",
        "    lemma = nlp.WordNetLemmatizer()\n",
        "    description = [ lemma.lemmatize(word) for word in description]\n",
        "    description = \" \".join(description)\n",
        "    description_list.append(description)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZBGlFp0Q2NW",
        "colab_type": "text"
      },
      "source": [
        "Создаем bag-of-words, для этого выбираем 3000 максимально встречаемых слов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH0OST_KQ4gj",
        "colab_type": "code",
        "outputId": "85c5d768-2faf-4981-bc84-92b5d19269b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "max_features = 3000\n",
        "count_vectorizer = CountVectorizer(max_features = max_features, stop_words = \"english\")\n",
        "sparce_matrix = count_vectorizer.fit_transform(description_list).toarray()\n",
        "print(\"Самые часто встречаемые {} слов: {}\".format(max_features,count_vectorizer.get_feature_names()))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Самые часто встречаемые 3000 слов: ['aah', 'aathi', 'abi', 'ability', 'abiola', 'abj', 'able', 'absolutly', 'abt', 'abta', 'aburo', 'ac', 'academic', 'acc', 'accept', 'access', 'accident', 'accidentally', 'accordingly', 'account', 'ache', 'acl', 'aco', 'acted', 'acting', 'action', 'activate', 'active', 'activity', 'actor', 'actual', 'actually', 'ad', 'adam', 'add', 'addamsfa', 'added', 'addicted', 'addie', 'address', 'admin', 'administrator', 'admirer', 'admit', 'adore', 'adoring', 'adult', 'advance', 'adventure', 'advice', 'advise', 'ae', 'aeronautics', 'aeroplane', 'affair', 'affection', 'afraid', 'aft', 'afternoon', 'aftr', 'ag', 'agalla', 'age', 'agent', 'ago', 'agree', 'ah', 'aha', 'ahead', 'ahmad', 'aid', 'aight', 'ain', 'aint', 'air', 'airport', 'airtel', 'aiya', 'aiyah', 'aiyar', 'aiyo', 'aj', 'aka', 'al', 'alaipayuthe', 'album', 'alcohol', 'alert', 'alex', 'alfie', 'algarve', 'ali', 'alive', 'allah', 'allow', 'allowed', 'alright', 'alrite', 'alwys', 'amazing', 'american', 'amp', 'amt', 'amused', 'amy', 'andros', 'angry', 'animation', 'anna', 'annie', 'anniversary', 'announcement', 'annoying', 'anot', 'ansr', 'answer', 'answered', 'answering', 'anthony', 'anti', 'anybody', 'anymore', 'anythin', 'anytime', 'anyways', 'aom', 'apart', 'apartment', 'apo', 'apologise', 'app', 'apparently', 'apple', 'applebees', 'application', 'apply', 'appointment', 'appreciate', 'appreciated', 'approx', 'apps', 'appt', 'april', 'aproach', 'ar', 'arcade', 'ard', 'area', 'aren', 'arent', 'argh', 'argue', 'argument', 'arm', 'armand', 'arng', 'arrange', 'arrested', 'arrive', 'arsenal', 'art', 'arun', 'asap', 'ashley', 'ask', 'askd', 'asked', 'askin', 'asking', 'asks', 'asleep', 'asp', 'assume', 'ate', 'atlanta', 'atlast', 'atm', 'attached', 'attempt', 'attend', 'auction', 'audition', 'audrey', 'august', 'aunt', 'aunty', 'auto', 'av', 'available', 'avatar', 'ave', 'avent', 'avoid', 'avoiding', 'await', 'awaiting', 'awake', 'award', 'awarded', 'away', 'awesome', 'aww', 'ayn', 'ba', 'babe', 'baby', 'bad', 'bag', 'bahamas', 'bak', 'balance', 'ball', 'bang', 'bank', 'bar', 'barely', 'base', 'basic', 'basically', 'bat', 'batch', 'bath', 'bathe', 'bathing', 'battery', 'battle', 'bay', 'bb', 'bbd', 'bc', 'bck', 'bcm', 'bcoz', 'bcums', 'bday', 'bear', 'beautiful', 'beauty', 'bec', 'becoz', 'bed', 'bedrm', 'bedroom', 'beer', 'befor', 'beg', 'begging', 'begin', 'behave', 'bein', 'believe', 'belive', 'bell', 'belly', 'belovd', 'beloved', 'ben', 'beneath', 'beneficiary', 'benefit', 'best', 'bet', 'better', 'beware', 'bf', 'bhaji', 'bid', 'big', 'bigger', 'biggest', 'billed', 'billion', 'bin', 'biola', 'bird', 'birla', 'birth', 'birthdate', 'birthday', 'bishan', 'bit', 'bitch', 'bite', 'biz', 'bk', 'black', 'blackberry', 'blah', 'blake', 'blame', 'blank', 'blanket', 'bleh', 'bless', 'blessed', 'blessing', 'blind', 'block', 'bloke', 'blonde', 'bloo', 'blood', 'bloody', 'bloomberg', 'blow', 'blu', 'blue', 'bluetooth', 'bluff', 'blur', 'bmw', 'boat', 'body', 'bold', 'bone', 'bonus', 'boo', 'book', 'booked', 'booking', 'boost', 'booty', 'bootydelious', 'bored', 'borin', 'boring', 'born', 'borrow', 'bos', 'boston', 'bother', 'bottle', 'bought', 'bout', 'bowl', 'box', 'boy', 'boye', 'boyfriend', 'boytoy', 'bp', 'brah', 'brain', 'brand', 'brandy', 'bray', 'bread', 'break', 'breath', 'breathe', 'brief', 'bright', 'brilliant', 'bring', 'bringing', 'brings', 'bristol', 'british', 'britney', 'bro', 'broad', 'broke', 'broken', 'bros', 'brothas', 'brother', 'brought', 'brownie', 'bruce', 'bruv', 'bslvyl', 'bstfrnd', 'bt', 'btw', 'buck', 'bud', 'buddy', 'budget', 'buff', 'buffet', 'bugis', 'build', 'building', 'bulb', 'bun', 'burger', 'burn', 'burning', 'bus', 'business', 'busy', 'butt', 'buy', 'buyer', 'buying', 'buzy', 'buzz', 'bx', 'bye', 'cabin', 'cafe', 'cake', 'cal', 'calculation', 'cali', 'calicut', 'california', 'callback', 'callcost', 'calld', 'called', 'caller', 'callertune', 'callfreefone', 'callin', 'calling', 'calm', 'cam', 'camcorder', 'came', 'camera', 'campus', 'canada', 'canal', 'canary', 'cancel', 'cancelled', 'cancer', 'cann', 'capital', 'cappuccino', 'captain', 'car', 'card', 'cardiff', 'care', 'cared', 'career', 'careful', 'carefully', 'caring', 'carlos', 'caroline', 'carry', 'cartoon', 'case', 'cash', 'cashbin', 'cashto', 'castor', 'cat', 'catch', 'catching', 'caught', 'cause', 'causing', 'cbe', 'cc', 'cd', 'cdgt', 'celeb', 'celebrate', 'celebration', 'cell', 'center', 'centre', 'certainly', 'ch', 'cha', 'chain', 'challenge', 'chance', 'change', 'changed', 'channel', 'character', 'charge', 'charged', 'charity', 'charles', 'chart', 'chase', 'chasing', 'chat', 'chatting', 'cheap', 'cheaper', 'cheat', 'cheating', 'chechi', 'check', 'checked', 'checking', 'cheer', 'chennai', 'chicken', 'chikku', 'child', 'childish', 'chill', 'chillin', 'china', 'chinese', 'choice', 'choose', 'chosen', 'christmas', 'church', 'cine', 'cinema', 'citizen', 'city', 'claim', 'claire', 'class', 'clean', 'cleaning', 'clear', 'cleared', 'click', 'clock', 'clos', 'close', 'closed', 'closer', 'club', 'cm', 'cn', 'cock', 'code', 'coffee', 'coin', 'cold', 'colleague', 'collect', 'collected', 'collecting', 'collection', 'college', 'colour', 'com', 'come', 'comedy', 'comin', 'coming', 'common', 'community', 'comp', 'company', 'competition', 'complete', 'completely', 'complimentary', 'computer', 'comuk', 'concentrate', 'concert', 'condition', 'confidence', 'confirm', 'confirmed', 'congrats', 'congratulation', 'connect', 'connection', 'considering', 'constantly', 'contact', 'contacted', 'content', 'contract', 'convey', 'cook', 'cooking', 'cool', 'copy', 'cornwall', 'correct', 'cost', 'costa', 'couldn', 'count', 'country', 'couple', 'course', 'cover', 'coz', 'cr', 'crab', 'crack', 'cramp', 'crave', 'crazy', 'cream', 'created', 'credit', 'credited', 'creepy', 'cricketer', 'crisis', 'crore', 'cross', 'croydon', 'cruise', 'csbcm', 'csh', 'ctxt', 'cud', 'cuddle', 'cuddling', 'cum', 'cup', 'curious', 'current', 'currently', 'curry', 'cust', 'custcare', 'custom', 'customer', 'cut', 'cute', 'cutefrnd', 'cuz', 'cw', 'da', 'dad', 'daddy', 'dai', 'daily', 'damn', 'dare', 'dark', 'darlin', 'darling', 'darren', 'dat', 'date', 'datebox', 'dating', 'datz', 'dave', 'day', 'dead', 'deal', 'dear', 'dearer', 'dearly', 'death', 'december', 'decide', 'decided', 'decimal', 'decision', 'deep', 'def', 'definitely', 'del', 'deleted', 'deliver', 'delivered', 'deliveredtomorrow', 'delivery', 'dem', 'den', 'depends', 'derek', 'dey', 'di', 'diamond', 'dick', 'dictionary', 'did', 'didn', 'didnt', 'die', 'died', 'diet', 'diff', 'difference', 'different', 'difficult', 'dificult', 'digital', 'dignity', 'din', 'dinner', 'dint', 'direct', 'directly', 'dirty', 'dis', 'discount', 'discus', 'dislike', 'display', 'distance', 'disturb', 'dload', 'dnt', 'doc', 'doctor', 'doe', 'doesn', 'doesnt', 'dog', 'dogging', 'doggy', 'doin', 'doing', 'dollar', 'don', 'dont', 'door', 'dot', 'double', 'download', 'downloads', 'draw', 'dream', 'dress', 'dressed', 'dresser', 'drink', 'drinking', 'drive', 'drivin', 'driving', 'drop', 'dropped', 'drug', 'drunk', 'dry', 'dsn', 'dubsack', 'dude', 'dun', 'dunno', 'dvd', 'ear', 'earlier', 'early', 'earth', 'easier', 'easy', 'eat', 'eaten', 'eatin', 'eating', 'ec', 'eerie', 'effect', 'egg', 'eh', 'em', 'email', 'embarassed', 'end', 'ended', 'ending', 'enemy', 'energy', 'eng', 'england', 'english', 'enjoy', 'enjoyed', 'enter', 'entered', 'entitled', 'entry', 'enuff', 'envelope', 'er', 'erm', 'error', 'escape', 'ese', 'especially', 'esplanade', 'essential', 'euro', 'eve', 'evening', 'event', 'everybody', 'evn', 'evng', 'evry', 'ex', 'exact', 'exactly', 'exam', 'excellent', 'exciting', 'excuse', 'exe', 'executive', 'exhausted', 'expect', 'expecting', 'expensive', 'experience', 'expires', 'explain', 'express', 'extra', 'eye', 'fa', 'face', 'facebook', 'fact', 'failed', 'fair', 'fall', 'family', 'fan', 'fancy', 'fantastic', 'fantasy', 'far', 'farm', 'fast', 'faster', 'fat', 'father', 'fathima', 'fault', 'fave', 'favor', 'favour', 'favourite', 'fb', 'fear', 'feb', 'february', 'fee', 'feel', 'feelin', 'feeling', 'fell', 'felt', 'female', 'fetch', 'fever', 'fight', 'fighting', 'fightng', 'figure', 'file', 'film', 'final', 'finally', 'fine', 'finger', 'finish', 'finished', 'finishing', 'fish', 'fit', 'fix', 'fixed', 'fl', 'flag', 'flaked', 'flash', 'flat', 'flight', 'flirt', 'floor', 'flower', 'fly', 'fml', 'follow', 'followed', 'following', 'fone', 'food', 'fool', 'foot', 'football', 'footprint', 'force', 'foreign', 'forever', 'forevr', 'forget', 'forgot', 'form', 'forum', 'forward', 'forwarded', 'fr', 'fran', 'freak', 'free', 'freefone', 'freemsg', 'freephone', 'freezing', 'fren', 'frens', 'fri', 'friday', 'friend', 'friendship', 'frm', 'frnd', 'frnds', 'frndship', 'frog', 'fromm', 'frying', 'fuck', 'fucked', 'fuckin', 'fucking', 'ful', 'fullonsms', 'fun', 'function', 'funeral', 'funk', 'funky', 'funny', 'furniture', 'future', 'fwd', 'fyi', 'ga', 'gain', 'gal', 'galileo', 'game', 'gamestar', 'ganesh', 'gang', 'gap', 'garage', 'garbage', 'garden', 'gardener', 'gary', 'gas', 'gastroenteritis', 'gautham', 'gave', 'gay', 'gaytextbuddy', 'gb', 'gbp', 'gd', 'ge', 'gee', 'geeee', 'geeeee', 'gender', 'generally', 'genius', 'gent', 'gentle', 'gentleman', 'gently', 'genuine', 'george', 'germany', 'getstop', 'gettin', 'getting', 'getzed', 'geva', 'gf', 'ghost', 'gift', 'gim', 'girl', 'girlfrnd', 'giv', 'given', 'giving', 'glad', 'gm', 'gn', 'goal', 'god', 'goin', 'going', 'gon', 'gona', 'gone', 'good', 'goodmorning', 'goodnight', 'goodnite', 'google', 'gorgeous', 'gossip', 'got', 'goto', 'govt', 'gr', 'grahmbell', 'gram', 'grand', 'gravity', 'great', 'green', 'greet', 'greeting', 'grin', 'grl', 'ground', 'group', 'gt', 'guaranteed', 'gud', 'gudnite', 'guess', 'guide', 'guilty', 'guy', 'gym', 'ha', 'haf', 'haha', 'hahaha', 'hai', 'hair', 'haiz', 'half', 'halloween', 'ham', 'hand', 'handed', 'handle', 'handset', 'hanging', 'happen', 'happend', 'happened', 'happening', 'happens', 'happiness', 'happy', 'hard', 'hardcore', 'harry', 'hasn', 'hate', 'hav', 'haven', 'havent', 'havin', 'having', 'havnt', 'head', 'headache', 'hear', 'heard', 'heart', 'heavy', 'hee', 'height', 'helen', 'hell', 'hella', 'hello', 'help', 'hey', 'hg', 'hi', 'hide', 'high', 'hill', 'hint', 'hip', 'history', 'hit', 'hiya', 'hl', 'hmm', 'hmmm', 'hmv', 'ho', 'hold', 'holder', 'holding', 'holiday', 'holla', 'hols', 'home', 'homeowner', 'hon', 'honey', 'honeybee', 'hook', 'hop', 'hope', 'hopefully', 'hoping', 'horny', 'horrible', 'hospital', 'hostel', 'hot', 'hotel', 'hour', 'house', 'hows', 'howz', 'hp', 'hr', 'http', 'hubby', 'hug', 'huh', 'hun', 'hungry', 'hunny', 'hurry', 'hurt', 'husband', 'hv', 'hw', 'iam', 'ibhltd', 'ibiza', 'ic', 'ice', 'id', 'idea', 'identifier', 'idiot', 'idk', 'ignore', 'ikea', 'il', 'ill', 'im', 'imagine', 'imma', 'immediately', 'important', 'impossible', 'inch', 'incident', 'include', 'including', 'inclusive', 'india', 'indian', 'infernal', 'info', 'inform', 'information', 'informed', 'inning', 'insha', 'inside', 'instantly', 'instead', 'instituitions', 'instruction', 'insurance', 'intelligent', 'intention', 'interested', 'interesting', 'interflora', 'internet', 'interview', 'intro', 'invader', 'invest', 'invite', 'invited', 'inviting', 'invnted', 'iouri', 'ip', 'ipad', 'ipod', 'iq', 'irritates', 'irritating', 'iscoming', 'ish', 'island', 'isn', 'isnt', 'issue', 'italian', 'itcould', 'item', 'itwhichturnedinto', 'itz', 'ive', 'iz', 'izzit', 'ja', 'jacket', 'jackpot', 'jada', 'james', 'jamster', 'jan', 'jane', 'january', 'japanese', 'jas', 'jason', 'java', 'jay', 'jaya', 'jazz', 'jd', 'jealous', 'jean', 'jen', 'jenny', 'jerry', 'jess', 'jesus', 'jhl', 'jia', 'jiayin', 'jiu', 'jo', 'joanna', 'job', 'jogging', 'john', 'join', 'joined', 'joining', 'joke', 'jokin', 'joking', 'jolly', 'jolt', 'jordan', 'journey', 'joy', 'jsco', 'jst', 'jstfrnd', 'jsut', 'juan', 'juicy', 'july', 'june', 'jus', 'just', 'juz', 'kadeem', 'kaiez', 'kallis', 'kano', 'kappa', 'karaoke', 'kate', 'kavalan', 'kay', 'kb', 'ke', 'keeping', 'kegger', 'kent', 'kept', 'kerala', 'keralacircle', 'kettoda', 'key', 'kg', 'kick', 'kickoff', 'kid', 'kidding', 'kidz', 'kill', 'killed', 'killing', 'kind', 'kinda', 'kindly', 'king', 'kiosk', 'kiss', 'kl', 'knackered', 'knee', 'knew', 'knock', 'know', 'knowing', 'knw', 'konw', 'kothi', 'kr', 'kudi', 'kusruthi', 'kz', 'la', 'lab', 'lac', 'lady', 'lag', 'laid', 'land', 'landline', 'lane', 'langport', 'language', 'laptop', 'lar', 'largest', 'late', 'lately', 'later', 'latest', 'latr', 'laugh', 'laughed', 'laughing', 'laundry', 'law', 'lay', 'lazy', 'lccltd', 'ldew', 'ldn', 'ldnw', 'le', 'lead', 'leaf', 'learn', 'leave', 'leaving', 'lect', 'lecture', 'left', 'leg', 'legal', 'leh', 'lei', 'lem', 'length', 'leona', 'lesson', 'let', 'letter', 'lf', 'liao', 'lib', 'library', 'lick', 'lido', 'lie', 'life', 'lifetime', 'lifpartnr', 'lift', 'light', 'lik', 'like', 'liked', 'likely', 'lil', 'lily', 'limit', 'limiting', 'line', 'linerental', 'link', 'lion', 'lionm', 'lionp', 'lip', 'list', 'listen', 'listening', 'literally', 'little', 'live', 'lived', 'liverpool', 'living', 'lk', 'll', 'lmao', 'lo', 'load', 'loan', 'local', 'location', 'lock', 'log', 'login', 'logo', 'lol', 'london', 'lonely', 'long', 'longer', 'look', 'lookatme', 'looked', 'lookin', 'looking', 'loose', 'lor', 'lose', 'loses', 'losing', 'loss', 'lost', 'lot', 'lotr', 'lotta', 'lou', 'loud', 'lounge', 'lousy', 'lov', 'lovable', 'love', 'loved', 'lovejen', 'lovely', 'loveme', 'lover', 'loverboy', 'loving', 'lovingly', 'low', 'lower', 'loxahatchee', 'loyal', 'loyalty', 'lp', 'lst', 'lt', 'lttrs', 'luck', 'lucky', 'lucozade', 'lucy', 'lunch', 'lush', 'luv', 'luvs', 'lux', 'luxury', 'lv', 'lvblefrnd', 'lyf', 'lyfu', 'lyk', 'ma', 'maangalyam', 'mac', 'machan', 'macho', 'mad', 'madam', 'mag', 'maga', 'magical', 'mah', 'mahal', 'maid', 'mail', 'mailbox', 'main', 'maintain', 'major', 'make', 'makin', 'making', 'malaria', 'male', 'mall', 'man', 'manage', 'managed', 'management', 'manda', 'mandan', 'maneesha', 'map', 'march', 'margaret', 'mark', 'market', 'marriage', 'married', 'marrow', 'marry', 'massage', 'massive', 'master', 'match', 'mate', 'math', 'mathematics', 'matrix', 'matter', 'matured', 'maturity', 'max', 'maximize', 'mayb', 'maybe', 'mb', 'mca', 'mcat', 'meal', 'mean', 'meaning', 'meant', 'measure', 'med', 'medical', 'medicine', 'meet', 'meetin', 'meeting', 'mega', 'meh', 'mei', 'mel', 'melle', 'melt', 'member', 'membership', 'memory', 'men', 'mental', 'menu', 'meow', 'merry', 'mesages', 'mess', 'message', 'messaged', 'messaging', 'messenger', 'messy', 'met', 'mi', 'mid', 'middle', 'midnight', 'mids', 'mila', 'mile', 'milk', 'million', 'min', 'mind', 'mini', 'minimum', 'minmobsmorelkpobox', 'minmoremobsemspobox', 'minnaminunginte', 'minor', 'minute', 'minuts', 'miracle', 'misbehaved', 'miserable', 'miss', 'missed', 'missin', 'missing', 'mistake', 'mite', 'mitsake', 'mix', 'mk', 'ml', 'mm', 'mmm', 'mmmm', 'mmmmm', 'mmmmmm', 'mnth', 'mnths', 'mo', 'moan', 'mob', 'mobile', 'mobilesdirect', 'mobileupd', 'mobno', 'moby', 'mode', 'model', 'module', 'moji', 'mojibiola', 'mokka', 'mom', 'moment', 'mon', 'monday', 'money', 'monkey', 'mono', 'month', 'monthly', 'mood', 'moon', 'moral', 'morefrmmob', 'morn', 'mornin', 'morning', 'moro', 'morow', 'morphine', 'morro', 'morrow', 'mother', 'motorola', 'mountain', 'mouth', 'moved', 'movie', 'movietrivia', 'moving', 'mp', 'mr', 'mrng', 'mrt', 'mrw', 'msg', 'msging', 'msgrcvd', 'msgrcvdhg', 'msn', 'mt', 'mtalk', 'mth', 'mths', 'mtmsg', 'mtmsgrcvd', 'mu', 'muah', 'mum', 'mummy', 'mumtaz', 'munsters', 'murder', 'murdered', 'murderer', 'music', 'musthu', 'muz', 'mystery', 'na', 'nag', 'nagar', 'nah', 'nahi', 'naked', 'nalla', 'named', 'nan', 'nanny', 'nap', 'nasdaq', 'nasty', 'nat', 'natalie', 'natalja', 'national', 'natural', 'nature', 'naughty', 'nb', 'nd', 'ne', 'near', 'nearly', 'necessarily', 'necessary', 'neck', 'necklace', 'ned', 'need', 'needed', 'neft', 'neighbor', 'neighbour', 'nervous', 'net', 'netcollex', 'network', 'networking', 'neva', 'new', 'neway', 'newest', 'news', 'ni', 'nic', 'nice', 'nichols', 'nigeria', 'night', 'nimya', 'nit', 'nite', 'nitros', 'noe', 'nok', 'nokia', 'nokias', 'noline', 'noon', 'nope', 'norm', 'normal', 'normally', 'northampton', 'note', 'nothin', 'notice', 'notxt', 'noun', 'nowadays', 'nt', 'ntt', 'ntwk', 'nu', 'num', 'number', 'nurungu', 'nuther', 'nvm', 'nw', 'nxt', 'ny', 'nyc', 'nydc', 'nyt', 'obviously', 'occupy', 'odi', 'offer', 'office', 'official', 'officially', 'ofice', 'oh', 'oi', 'oic', 'oil', 'ok', 'okay', 'okey', 'okie', 'ola', 'old', 'omg', 'omw', 'oni', 'onion', 'online', 'onwards', 'oooh', 'oops', 'open', 'opening', 'operator', 'opinion', 'opportunity', 'opt', 'option', 'optout', 'orange', 'orchard', 'order', 'ordered', 'oredi', 'oreo', 'orig', 'original', 'oru', 'oso', 'otside', 'outage', 'outside', 'outstanding', 'outta', 'ovulation', 'ow', 'owns', 'oz', 'pa', 'pack', 'package', 'page', 'paid', 'pain', 'painful', 'painting', 'pale', 'pan', 'pandy', 'panic', 'pap', 'paper', 'paperwork', 'paragon', 'parco', 'parent', 'paris', 'park', 'parked', 'parking', 'partner', 'partnership', 'party', 'pas', 'passed', 'passionate', 'password', 'past', 'path', 'pattern', 'patty', 'pay', 'payed', 'payee', 'paying', 'payment', 'payoh', 'pc', 'peace', 'peaceful', 'peak', 'pee', 'pen', 'pending', 'penis', 'penny', 'people', 'percent', 'perfect', 'period', 'permission', 'person', 'personal', 'personality', 'perwksub', 'pete', 'petey', 'petrol', 'pg', 'ph', 'philosophy', 'phne', 'phoenix', 'phone', 'phoned', 'photo', 'php', 'pic', 'pick', 'picked', 'picking', 'pickle', 'picsfree', 'picture', 'pie', 'piece', 'pig', 'pilate', 'pimple', 'pin', 'pink', 'piss', 'pissed', 'pix', 'pizza', 'place', 'placement', 'plan', 'plane', 'planet', 'planned', 'planning', 'play', 'played', 'player', 'playing', 'plaza', 'pleased', 'pleasure', 'plenty', 'plm', 'pls', 'plus', 'plz', 'pm', 'po', 'pobox', 'pocketbabe', 'pod', 'poem', 'point', 'poker', 'pole', 'police', 'politician', 'polo', 'poly', 'polyh', 'polyph', 'polyphonic', 'polys', 'pongal', 'pool', 'poop', 'poor', 'pop', 'popcorn', 'popped', 'porn', 'position', 'possession', 'possible', 'post', 'postcard', 'postcode', 'posted', 'potato', 'potential', 'potter', 'pouch', 'pound', 'pours', 'pout', 'power', 'pp', 'ppermesssubscription', 'ppl', 'pple', 'ppm', 'ppmx', 'ppw', 'prabha', 'practical', 'practice', 'practicing', 'pray', 'praying', 'pre', 'prefer', 'preferably', 'prem', 'premier', 'premium', 'prepaid', 'prepare', 'prepared', 'prepayment', 'prescription', 'present', 'press', 'pretty', 'previous', 'previously', 'prey', 'price', 'pride', 'prince', 'princess', 'print', 'printed', 'priscilla', 'privacy', 'private', 'prize', 'pro', 'prob', 'probably', 'problem', 'probs', 'process', 'processed', 'prof', 'professor', 'profile', 'profit', 'program', 'project', 'prolly', 'promise', 'promo', 'prompt', 'proof', 'properly', 'property', 'propose', 'propsd', 'prospect', 'protect', 'prove', 'proverb', 'provided', 'pt', 'ptbo', 'pub', 'public', 'pull', 'purchase', 'purity', 'purpose', 'purse', 'push', 'pussy', 'puttin', 'putting', 'pw', 'px', 'qatar', 'qp', 'qu', 'quality', 'queen', 'ques', 'question', 'questioned', 'quick', 'quickly', 'quiet', 'quit', 'quite', 'quiz', 'quote', 'quoting', 'qxj', 'racing', 'radio', 'raed', 'rael', 'railway', 'rain', 'raining', 'raise', 'raj', 'raji', 'rakhesh', 'rally', 'ran', 'random', 'randomly', 'randy', 'rang', 'range', 'ranjith', 'rate', 'ray', 'rcv', 'rcvd', 'rd', 'reach', 'reached', 'reaching', 'reaction', 'read', 'reader', 'reading', 'ready', 'real', 'realise', 'reality', 'realize', 'realized', 'really', 'realy', 'reason', 'reasonable', 'reboot', 'rec', 'recd', 'receipt', 'receive', 'receivea', 'received', 'receiving', 'recent', 'recently', 'recession', 'recharge', 'reckon', 'recognise', 'record', 'recovery', 'red', 'redeemed', 'reduce', 'ref', 'reference', 'refilled', 'refused', 'reg', 'regard', 'regarding', 'register', 'registered', 'regret', 'regular', 'relation', 'relative', 'relax', 'released', 'rem', 'remain', 'remains', 'remember', 'remembered', 'remembr', 'remind', 'reminder', 'reminding', 'removal', 'remove', 'removed', 'renewal', 'rent', 'rental', 'rentl', 'repair', 'repeat', 'replace', 'replacement', 'replied', 'reply', 'replying', 'report', 'representative', 'request', 'research', 'reserve', 'respect', 'respectful', 'responce', 'respond', 'responding', 'response', 'responsibility', 'rest', 'restaurant', 'result', 'resume', 'retrieve', 'return', 'returned', 'reveal', 'revealed', 'reverse', 'review', 'revision', 'reward', 'rewarding', 'rg', 'rgds', 'rhythm', 'rice', 'rich', 'ride', 'right', 'rightly', 'ring', 'ringtone', 'ringtoneking', 'ringtones', 'risk', 'rite', 'river', 'road', 'roast', 'rock', 'rofl', 'roger', 'role', 'romantic', 'ron', 'room', 'roommate', 'rose', 'round', 'row', 'royal', 'rply', 'rr', 'rstm', 'ru', 'rub', 'rude', 'ruin', 'ruining', 'rule', 'rum', 'rumour', 'run', 'running', 'rush', 'rw', 'ryan', 'sac', 'sachin', 'sacrifice', 'sad', 'sae', 'safe', 'said', 'sake', 'salam', 'salary', 'sale', 'salon', 'sam', 'santa', 'sar', 'sarasota', 'sarcasm', 'sarcastic', 'sary', 'sat', 'sathya', 'satisfied', 'satisfy', 'saturday', 'saucy', 'savamob', 'save', 'saved', 'saw', 'say', 'saying', 'scared', 'scary', 'sch', 'schedule', 'school', 'science', 'scold', 'score', 'scoring', 'scotch', 'scotland', 'scotsman', 'scream', 'screamed', 'screaming', 'screen', 'scrounge', 'sd', 'se', 'sea', 'search', 'searching', 'season', 'seat', 'sec', 'second', 'secret', 'secretary', 'secretly', 'section', 'sed', 'seed', 'seeing', 'seen', 'select', 'selected', 'selection', 'self', 'selfish', 'sell', 'selling', 'sem', 'semester', 'sen', 'send', 'sender', 'sending', 'sends', 'sense', 'sensitive', 'sent', 'sentence', 'senthil', 'sept', 'series', 'seriously', 'service', 'serving', 'set', 'setting', 'settle', 'settled', 'seven', 'sex', 'sexy', 'sh', 'sha', 'shagged', 'shahjahan', 'shall', 'shame', 'shampain', 'share', 'shared', 'sharing', 'shd', 'sheet', 'sheffield', 'shelf', 'shesil', 'shijas', 'shining', 'ship', 'shipped', 'shipping', 'shirt', 'shit', 'shitload', 'shld', 'shock', 'shocking', 'shoe', 'shoot', 'shop', 'shoppin', 'shopping', 'shore', 'short', 'shortage', 'shorter', 'shortly', 'shot', 'shouldn', 'shouted', 'shoving', 'shower', 'showing', 'shracomorsglsuplt', 'shu', 'shuhui', 'shut', 'shy', 'si', 'sian', 'sib', 'sick', 'sigh', 'sight', 'sign', 'signing', 'silence', 'silent', 'silently', 'silver', 'sim', 'simple', 'simpler', 'simply', 'sinco', 'sing', 'singing', 'single', 'sip', 'sipix', 'sir', 'sister', 'sit', 'site', 'sitll', 'sitting', 'situation', 'siva', 'size', 'sk', 'skilgme', 'skillgame', 'skip', 'sky', 'skype', 'skyped', 'slap', 'slave', 'sleep', 'sleepin', 'sleeping', 'slept', 'slice', 'slipper', 'slow', 'slowly', 'sm', 'small', 'smart', 'smile', 'smiling', 'smoke', 'smoking', 'smth', 'sn', 'snake', 'snow', 'social', 'sofa', 'soft', 'software', 'sol', 'solve', 'somebody', 'somethin', 'song', 'sony', 'sonyericsson', 'soon', 'sooner', 'sore', 'sorrow', 'sorry', 'sort', 'sorted', 'sorting', 'sory', 'soryda', 'sound', 'soup', 'source', 'south', 'sp', 'space', 'spanish', 'spare', 'speak', 'special', 'specially', 'speechless', 'speed', 'spell', 'spend', 'spending', 'spent', 'spk', 'spl', 'spoke', 'spoken', 'spook', 'sport', 'spree', 'spring', 'sry', 'st', 'staff', 'stamp', 'stand', 'standard', 'standing', 'star', 'start', 'started', 'starting', 'starwars', 'statement', 'station', 'stay', 'staying', 'std', 'step', 'stock', 'stockport', 'stomach', 'stomp', 'stone', 'stop', 'stopped', 'stoptxt', 'store', 'storming', 'story', 'str', 'straight', 'stranger', 'street', 'stress', 'stretch', 'strike', 'strip', 'strong', 'stuck', 'student', 'study', 'studying', 'stuff', 'stupid', 'style', 'stylish', 'sub', 'subpoly', 'subscribe', 'subscribed', 'subscriber', 'subscription', 'successful', 'successfully', 'suck', 'sugar', 'suggest', 'suite', 'sum', 'summer', 'sun', 'sunday', 'sunny', 'sunshine', 'suntec', 'sup', 'super', 'superb', 'superior', 'supervisor', 'supply', 'support', 'supposed', 'suprman', 'sura', 'sure', 'surely', 'surfing', 'surprise', 'surprised', 'sw', 'sweet', 'sweetest', 'swimming', 'swing', 'switch', 'swt', 'swtheart', 'symbol', 'ta', 'table', 'tablet', 'taken', 'takin', 'taking', 'talent', 'talk', 'talking', 'tampa', 'tape', 'tariff', 'tat', 'taunton', 'taylor', 'tb', 'tc', 'tcr', 'tea', 'teach', 'teacher', 'team', 'tear', 'tease', 'teasing', 'technical', 'teeth', 'tel', 'tell', 'telling', 'telphone', 'temp', 'temple', 'tenant', 'tenerife', 'term', 'terrible', 'tessy', 'test', 'text', 'textcomp', 'texted', 'texting', 'textoperator', 'textpod', 'tf', 'th', 'thangam', 'thank', 'thanks', 'thanksgiving', 'thanx', 'thats', 'theatre', 'themob', 'theory', 'thing', 'think', 'thinkin', 'thinking', 'thk', 'thm', 'thnk', 'tho', 'thought', 'threat', 'throat', 'throw', 'tht', 'thts', 'thurs', 'thursday', 'ti', 'tick', 'ticket', 'tihs', 'til', 'till', 'time', 'timing', 'tired', 'tirupur', 'title', 'tkts', 'tlp', 'tm', 'tmr', 'tncs', 'toa', 'toclaim', 'today', 'tog', 'told', 'toll', 'tom', 'tomarrow', 'tomo', 'tomorrow', 'tone', 'tonight', 'tonite', 'took', 'torch', 'tot', 'total', 'totally', 'touch', 'touched', 'tough', 'tour', 'town', 'track', 'trade', 'traffic', 'train', 'training', 'transaction', 'transfer', 'transfered', 'travel', 'treat', 'treated', 'tree', 'tried', 'trip', 'trouble', 'truck', 'true', 'truffle', 'truly', 'trust', 'truth', 'try', 'trying', 'tsandcs', 'tscs', 'tsunami', 'tt', 'ttyl', 'tues', 'tuesday', 'tuition', 'turn', 'tv', 'twice', 'txt', 'txtauction', 'txtin', 'txting', 'txts', 'tyler', 'type', 'tyrone', 'ubi', 'ugh', 'uk', 'umma', 'ummmmmaah', 'unable', 'uncle', 'understand', 'understanding', 'understood', 'uni', 'unique', 'university', 'unless', 'unlimited', 'unredeemed', 'unsold', 'unsub', 'unsubscribe', 'update', 'upgrade', 'upload', 'upset', 'upto', 'ur', 'urawinner', 'ure', 'urgent', 'urgently', 'urgnt', 'url', 'urn', 'urself', 'usc', 'use', 'used', 'user', 'usf', 'using', 'usual', 'usually', 'uz', 'vaazhthukkal', 'vale', 'valentine', 'valid', 'valuable', 'value', 'valued', 'various', 'vary', 'vava', 'vday', 've', 'vega', 'vegetable', 'verified', 'verify', 'version', 'vettam', 'vewy', 'vid', 'video', 'videochat', 'videophones', 'vijay', 'vikky', 'village', 'violated', 'violence', 'violet', 'vip', 'virgin', 'visionsms', 'visit', 'visitor', 'viva', 'vivek', 'vl', 'voda', 'vodafone', 'vodka', 'voice', 'voicemail', 'vomit', 'vomiting', 'vote', 'voucher', 'vry', 'vth', 'vu', 'wa', 'wah', 'waheed', 'waht', 'wait', 'waited', 'waitin', 'waiting', 'wake', 'waking', 'wale', 'walk', 'walked', 'walking', 'wall', 'wallpaper', 'walmart', 'wan', 'wana', 'want', 'wanted', 'wanting', 'wap', 'warm', 'warner', 'warning', 'warranty', 'wasn', 'waste', 'wasted', 'wat', 'watch', 'watching', 'water', 'watever', 'wating', 'wats', 'wave', 'waxsto', 'way', 'wb', 'wc', 'weak', 'weakness', 'wear', 'wearing', 'weather', 'web', 'website', 'wed', 'wedding', 'wednesday', 'wee', 'weed', 'week', 'weekend', 'weekly', 'weigh', 'weight', 'weird', 'weirdest', 'welcome', 'welp', 'wen', 'went', 'wer', 'wesley', 'west', 'westlife', 'wet', 'whats', 'whatsup', 'whenevr', 'white', 'whn', 'whr', 'wicklow', 'wid', 'widelive', 'wif', 'wife', 'wifi', 'wihtuot', 'wil', 'willing', 'win', 'winaweek', 'winawk', 'wind', 'window', 'wine', 'winner', 'winning', 'wipro', 'wisdom', 'wise', 'wish', 'wishin', 'wishing', 'wiskey', 'wit', 'wiv', 'wk', 'wkend', 'wkent', 'wkg', 'wkly', 'wks', 'wld', 'wml', 'wn', 'wnt', 'wo', 'woke', 'woken', 'woman', 'won', 'wonder', 'wonderful', 'wondering', 'wont', 'woot', 'word', 'work', 'workin', 'working', 'world', 'worried', 'worry', 'worse', 'worst', 'worth', 'wot', 'woulda', 'wouldn', 'wow', 'wp', 'wq', 'wrc', 'write', 'wrk', 'wrnog', 'wrong', 'wrote', 'wt', 'wtf', 'wu', 'wud', 'wuld', 'wun', 'www', 'wx', 'wylie', 'xam', 'xavier', 'xchat', 'xh', 'xin', 'xmas', 'xn', 'xuhui', 'xx', 'xxx', 'xxxmobilemovieclub', 'xxxx', 'xxxxx', 'xxxxxx', 'xxxxxxx', 'xxxxxxxxx', 'xy', 'ya', 'yahoo', 'yan', 'yar', 'yarasu', 'yay', 'yck', 'yeah', 'year', 'yeh', 'yelling', 'yellow', 'yep', 'yer', 'yes', 'yest', 'yesterday', 'yetunde', 'yf', 'yijue', 'ym', 'yo', 'yoga', 'yogasana', 'yor', 'youre', 'yr', 'yummy', 'yun', 'yunny', 'yuo', 'yup', 'zed', 'zindgi', 'zoe']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbDY56XWV7do",
        "colab_type": "text"
      },
      "source": [
        "Исходные данные преобразуем в bag-of-words формат"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8yCXDOfWAww",
        "colab_type": "code",
        "outputId": "cc17a030-489c-48bc-9925-be325f67ef4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "sparce_matrix[:4]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e24S-P4Vsrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = data.iloc[:,0].values\n",
        "x = sparce_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCv03et0WW5L",
        "colab_type": "text"
      },
      "source": [
        "Делим данные на тренировочные и тестовые"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF67b7VdV2VK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFi1KFI8WaWS",
        "colab_type": "text"
      },
      "source": [
        "Напишем наивный баесовский классификатор"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB_povh6hYuU",
        "colab_type": "text"
      },
      "source": [
        "# Баесовский метод решения классификационной проблемы.\n",
        "\n",
        "Лучшим другом аналитика данных является теорема Байеса4, которая позволяет \"переставить\" условные вероятности местами. Пусть нужно узнать вероятность не коего события E, зависящего от наступления некоего другого события F, причем в наличии имеется лишь информация о вероятности события F, зависящего от наступления события E. Двукратное применение (в силу симметрии) определения условной вероятности даст формулу Байеса:\n",
        "\n",
        "\n",
        "$$ P(A\\mid B) = \\frac{P(B\\mid A) P(A)}{P(B)}$$\n",
        "\n",
        "где $P(A\\mid B)$ - вероятность наступления события A при условии наличия события B\n",
        "\n",
        "\n",
        "Если событие B разложить на два взаимоисключающих события B при условии A и B при условии $\\bar{E}$, то событие P(B) можно представить как сумма вероятностей наступления событий $P(B\\mid A)$ и  $P(B\\mid \\bar{A})$, тогда формула вероятности примет вид:\n",
        "\n",
        "$$P(A\\mid B) = \\frac{P(B\\mid A) P(A)}{P(B\\mid A)P(A) + P(B\\mid \\bar{A})P(\\bar{A})}$$\n",
        "\n",
        "\n",
        "Если события независимы:\n",
        "\n",
        "$$P(A\\mid B) = P(A)P(B)$$\n",
        "\n",
        "Если события зависимы, и при этом вероятность B не равна нулю, то \n",
        "\n",
        "$$P(A\\mid B) = \\frac{P(A, B) }{P(B)}$$\n",
        "\n",
        "\n",
        "Под этим подразумевается вероятность наступления события A при условии, что известно о наступлении события B.\n",
        "\n",
        "В случае независимости двух переменных формула принимает вид:\n",
        "\n",
        "$$P(A\\mid B) = P(A)$$\n",
        "\n",
        "\n",
        "означает, что наличие наступления события B не дает нам никакой информации о наступлении события A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls-QwJfpWNVH",
        "colab_type": "code",
        "outputId": "3e25c519-f997-4dce-a5a9-a0eb9ff61b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "nb = GaussianNB()\n",
        "nb.fit(x_train,y_train)\n",
        "print(\"the accuracy of our model: {}\".format(nb.score(x_test,y_test)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the accuracy of our model: 0.8763440860215054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9D5KTE6h19S",
        "colab_type": "code",
        "outputId": "2e906447-a5c7-4f34-af41-fae558d552b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, nb.predict(x_test)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.88      0.93       485\n",
            "           1       0.52      0.84      0.64        73\n",
            "\n",
            "    accuracy                           0.88       558\n",
            "   macro avg       0.74      0.86      0.78       558\n",
            "weighted avg       0.91      0.88      0.89       558\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRdUt0ThWe30",
        "colab_type": "text"
      },
      "source": [
        "Напишем логистическую регрессию"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epQOa7FRWhqS",
        "colab_type": "code",
        "outputId": "6b2d2142-c3b5-44ea-d89f-cc81cfe3b768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(max_iter = 200)\n",
        "lr.fit(x_train,y_train)\n",
        "print(\"our accuracy is: {}\".format(lr.score(x_test,y_test)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "our accuracy is: 0.9767025089605734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNyFu4X1iabe",
        "colab_type": "code",
        "outputId": "287c036a-6a2e-4573-ed0a-118806e41f98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_test, lr.predict(x_test)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99       485\n",
            "           1       1.00      0.82      0.90        73\n",
            "\n",
            "    accuracy                           0.98       558\n",
            "   macro avg       0.99      0.91      0.94       558\n",
            "weighted avg       0.98      0.98      0.98       558\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWc_Fs10W71Y",
        "colab_type": "text"
      },
      "source": [
        "Классификатор по методу ближайшего соседа"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-05vUVlPW6Ey",
        "colab_type": "code",
        "outputId": "43917378-f049-4b4e-b1db-873b50d41469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "knn.fit(x_train,y_train)\n",
        "#print('Prediction: {}'.format(prediction))\n",
        "print('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With KNN (K=3) accuracy is:  0.942652329749104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIrdoC-7igLs",
        "colab_type": "code",
        "outputId": "ae02b5d9-1af4-4a88-e9e1-70a8d807ab89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_test, knn.predict(x_test)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      1.00      0.97       485\n",
            "           1       1.00      0.56      0.72        73\n",
            "\n",
            "    accuracy                           0.94       558\n",
            "   macro avg       0.97      0.78      0.84       558\n",
            "weighted avg       0.95      0.94      0.94       558\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSGytE8OisyJ",
        "colab_type": "text"
      },
      "source": [
        "Из всех выбранных моделей лучше всего дала результаты модель логистической регрессии."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5toCBH_mp61",
        "colab_type": "text"
      },
      "source": [
        "# Анализ текста на тональность"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-pDViybnhoU",
        "colab_type": "text"
      },
      "source": [
        "Рассмотрим датасет twitter sentiment analyses hatred speach https://www.kaggle.com/arkhoshghalb/twitter-sentiment-analysis-hatred-speech#train.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3EBuuYhmkVX",
        "colab_type": "code",
        "outputId": "2a3df272-4b2f-4e8b-a8ba-75e095afb386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "url='http://yustiks.ru/dataset/twitter_train.csv'\n",
        "s=requests.get(url).content\n",
        "data=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
        "data.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  label                                              tweet\n",
              "0   1      0   @user when a father is dysfunctional and is s...\n",
              "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
              "2   3      0                                bihday your majesty\n",
              "3   4      0  #model   i love u take with u all the time in ...\n",
              "4   5      0             factsguide: society now    #motivation"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DiMSMWyn2Fh",
        "colab_type": "text"
      },
      "source": [
        "Есть колонка label - класс 1 означает, что текст содержит в себе ненависть и расизм. 0 - текст нейтрален по теме."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri6UgBXQoBnb",
        "colab_type": "text"
      },
      "source": [
        "Задача - определить класс, к которому относится тот или иной текст"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnbwL_5mrS8U",
        "colab_type": "text"
      },
      "source": [
        "Удаляем слова, которые не имеют смысловой нагрузки (например, слова 'и', 'или', 'а' и другие)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQvN2BL-ojtl",
        "colab_type": "code",
        "outputId": "752cb4db-bae2-4425-d851-18415bdc0dda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "import re\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def remove_stopwords(line):\n",
        "    word_tokens = word_tokenize(line)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "    return \" \".join(filtered_sentence)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbMjGkOgr4QA",
        "colab_type": "text"
      },
      "source": [
        "Далее функция, с помощью которой мы будем обрабатывать твиты\n",
        "\n",
        "\n",
        "*   переводим все слова в строчные буквы\n",
        "*   удаляем цифры\n",
        "*   удаляем пунктуацию\n",
        "*   удаляем стоп-слова\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qfvy75XrDtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(line):\n",
        "  # все слова переводим в строчный текст\n",
        "    line = line.lower()\n",
        "  # удаляем цифры\n",
        "    line = re.sub(r'\\d+', '', line)\n",
        "  # удаляем пунктуацию\n",
        "    line = line.translate(line.maketrans(\"\",\"\", string.punctuation))\n",
        "    line = remove_stopwords(line)\n",
        "    return line\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-I5nlmNsmpZ",
        "colab_type": "text"
      },
      "source": [
        "Предобработка всех твитов из таблицы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHtyWq8NrA8H",
        "colab_type": "code",
        "outputId": "c0031080-2581-415d-c87b-7cf68bb118c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "train = data\n",
        "\n",
        "for i,line in enumerate(train.tweet):\n",
        "    train.tweet[i] = preprocess(line)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqH38PT5sb47",
        "colab_type": "text"
      },
      "source": [
        "Разделим датасет на тренировочный и тестовый"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqF08sLBsapA",
        "colab_type": "code",
        "outputId": "4493f019-5896-4288-cfd8-c07b7ce88e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train['tweet'], train['label'], test_size=0.5, stratify=train['label'])\n",
        "\n",
        "trainp=train[train.label==1]\n",
        "trainn=train[train.label==0]\n",
        "print(trainp.info())\n",
        "trainn.info()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2242 entries, 13 to 31960\n",
            "Data columns (total 3 columns):\n",
            "id       2242 non-null int64\n",
            "label    2242 non-null int64\n",
            "tweet    2242 non-null object\n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 70.1+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 29720 entries, 0 to 31961\n",
            "Data columns (total 3 columns):\n",
            "id       29720 non-null int64\n",
            "label    29720 non-null int64\n",
            "tweet    29720 non-null object\n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 928.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEBfXJvFstxJ",
        "colab_type": "text"
      },
      "source": [
        "Можно заметить, что классы **несбалансированы**: в классе 1 2242 элемента, а в классе 0 их 29720. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZzvozfItINL",
        "colab_type": "text"
      },
      "source": [
        "Создадим bag-of-words вектора для всех твитов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNcOfFrDtNoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "vect = CountVectorizer()\n",
        "tf_train=vect.fit_transform(X_train)  #train the vectorizer, build the vocablury\n",
        "tf_test=vect.transform(X_test)  #get same encodings on test data as of vocabulary built"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz_uoxWUtRJl",
        "colab_type": "text"
      },
      "source": [
        "Создадим модель **Наивный баес**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcwwEUNHtTA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model = MultinomialNB()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdihWKR6tlbR",
        "colab_type": "text"
      },
      "source": [
        "Обучим модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpthYn7htjiG",
        "colab_type": "code",
        "outputId": "9f1765a0-18dd-427e-d21b-c9fce9d0427e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.fit(X=tf_train,y=y_train)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOKUfO3JtoKC",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим качество модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x--lPL7Jtosu",
        "colab_type": "code",
        "outputId": "01116db4-9faa-4898-93f7-a64225fdf783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "expected = y_test\n",
        "predicted=model.predict(tf_test)\n",
        "from sklearn import metrics\n",
        "\n",
        "print(metrics.classification_report(expected, predicted))\n",
        "print(metrics.confusion_matrix(expected, predicted))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98     14860\n",
            "           1       0.89      0.44      0.59      1121\n",
            "\n",
            "    accuracy                           0.96     15981\n",
            "   macro avg       0.93      0.72      0.78     15981\n",
            "weighted avg       0.95      0.96      0.95     15981\n",
            "\n",
            "[[14799    61]\n",
            " [  624   497]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5hNKWCVtzkb",
        "colab_type": "text"
      },
      "source": [
        "Можно заметить, что класс 1 предсказывается намного хуже, чем класс 0: класса 1 намного меньше по числу элементов, чем класс 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkZX9ROIt_1f",
        "colab_type": "text"
      },
      "source": [
        "Сбалансируем датасет"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnkYC-snuOHP",
        "colab_type": "code",
        "outputId": "b11a54bc-0fc6-49de-e39a-65d7167c3914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "train_imbalanced = train\n",
        "from sklearn.utils import resample\n",
        "df_majority = train[train.label==0]\n",
        "df_minority = train[train.label==1]\n",
        " \n",
        "# Upsample minority class\n",
        "df_minority_upsampled = resample(df_minority, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=len(df_majority),    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        " \n",
        "# Combine majority class with upsampled minority class\n",
        "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        " \n",
        "# Display new class counts\n",
        "print(\"Before\")\n",
        "print(train.label.value_counts())\n",
        "print(\"After\")\n",
        "print(df_upsampled.label.value_counts())\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_upsampled['tweet'], df_upsampled['label'], test_size=0.5, stratify=df_upsampled['label'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before\n",
            "0    29720\n",
            "1     2242\n",
            "Name: label, dtype: int64\n",
            "After\n",
            "1    29720\n",
            "0    29720\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwutTun8xX2I",
        "colab_type": "text"
      },
      "source": [
        "Можно заметить, что тренировочных данных стало больше, и классы уравнялись в количестве."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoD2ivRazam6",
        "colab_type": "code",
        "outputId": "0fa5bda8-4a4c-46b9-9000-d8d5d577ed7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "tf_train=vect.fit_transform(X_train)\n",
        "tf_test=vect.transform(X_test)\n",
        "model.fit(X=tf_train,y=y_train)\n",
        "expected = y_test\n",
        "predicted=model.predict(tf_test)\n",
        "from sklearn import metrics\n",
        "\n",
        "print(metrics.classification_report(expected, predicted))\n",
        "print('Матрица confusion')\n",
        "print(metrics.confusion_matrix(expected, predicted))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.91      0.94     14860\n",
            "           1       0.92      0.98      0.95     14860\n",
            "\n",
            "    accuracy                           0.95     29720\n",
            "   macro avg       0.95      0.95      0.95     29720\n",
            "weighted avg       0.95      0.95      0.95     29720\n",
            "\n",
            "Матрица confusion\n",
            "[[13534  1326]\n",
            " [  259 14601]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GA-HqiesgQGd"
      },
      "source": [
        "Можно заметить, что балансировка привела к улучшению результата."
      ]
    }
  ]
}